{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e53cfc",
   "metadata": {},
   "source": [
    "# Self-Obfuscation Probe Experiments\n",
    "\n",
    "This notebook demonstrates how to use the self-obfuscation probe testing toolkit to analyze\n",
    "the mechanisms behind self-obfuscation in language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Import modules from the toolkit\n",
    "from models import (\n",
    "    load_local_model,\n",
    "    stitch_model,\n",
    "    unstitch_model,\n",
    "    get_model_reps,\n",
    "    unembed_probs,\n",
    ")\n",
    "from probes import load_word_to_probe_dict, get_probe_score, get_dist_between_reps\n",
    "from utils import layer_sweep, patching_exp, get_first_word_tokens_pos\n",
    "from data_utils import load_test_data, create_example_variations\n",
    "from visualization import (\n",
    "    plot_layer_sweep_results,\n",
    "    plot_stitching_results,\n",
    "    plot_representation_norms,\n",
    "    plot_kl_divergence,\n",
    "    plot_top_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9964c019",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "First, we'll set up the configuration for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ccfa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and configuration\n",
    "WORK_DIR = Path(\".\")\n",
    "\n",
    "# Using the exact paths from probe_testing_toolkit.py\n",
    "test_words = [\n",
    "    \"gun\",\n",
    "    \"apple\",\n",
    "    \"elephant\",\n",
    "    \"SQL\",\n",
    "    \"cloud\",\n",
    "    \"jester\",\n",
    "    \"mathematical\",\n",
    "    \"samantha\",\n",
    "]\n",
    "\n",
    "PROBES_CHECKPOINT_PATH = (\n",
    "    WORK_DIR\n",
    "    / \"probes_20250228_053913_gemma_2_9b_instruct_plr_4e-05_l12_logistic_ptwf_0.5_ps_400_ms_200_mlr_2e-06_bl_0.1_finetuned\"\n",
    ")\n",
    "MODEL_CHECKPOINT_PATH = (\n",
    "    WORK_DIR\n",
    "    / \"20250228_053913_gemma_2_9b_instruct_plr_4e-05_l12_logistic_ptwf_0.5_ps_400_ms_200_mlr_2e-06_bl_0.1_good\"\n",
    ")\n",
    "TEST_DATA_PATH = (\n",
    "    WORK_DIR / \"conditional_test_data_gemma_2_9b_instruct_20250226_010745.json\"\n",
    ")\n",
    "USE_RESPONSE_TOKEN_IDS = True\n",
    "LAYERS_TO_PROBE = [12]\n",
    "\n",
    "# Create results directory\n",
    "RESULTS_DIR = WORK_DIR / \"notebook_results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Define experiment parameters\n",
    "LAYERS_TO_PROBE = [12]  # Could also get this from the probe metadata\n",
    "TEST_WORDS = [\"gun\", \"bomb\", \"elephant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e7213",
   "metadata": {},
   "source": [
    "## Load Resources\n",
    "\n",
    "Now we'll load the models, probes, and test data needed for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load probes\n",
    "print(\"Loading probes...\")\n",
    "word_to_probe = load_word_to_probe_dict(PROBES_CHECKPOINT_PATH)\n",
    "print(f\"Loaded probes for words: {list(word_to_probe.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163df814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "print(\"Loading self-obfuscated model...\")\n",
    "self_obf_model = load_local_model(checkpoint_path=MODEL_CHECKPOINT_PATH)\n",
    "print(\"Loading base model...\")\n",
    "base_model = load_local_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dcc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "test_data_df = load_test_data(TEST_DATA_PATH)\n",
    "print(f\"Loaded {len(test_data_df)} test examples\")\n",
    "\n",
    "# Create example variations\n",
    "entries = create_example_variations(test_data_df, words=TEST_WORDS)\n",
    "print(f\"Created {len(entries)} example variations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c44b08",
   "metadata": {},
   "source": [
    "## 1. Baseline Comparisons\n",
    "\n",
    "First, let's compare the probe scores between the base and self-obfuscated models\n",
    "for different types of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline comparisons\n",
    "results = {}\n",
    "\n",
    "for entry_name, entry in entries.items():\n",
    "    base_score = (\n",
    "        get_probe_score(word_to_probe[\"gun\"], entry, model=base_model).mean().item()\n",
    "    )\n",
    "    obf_score = (\n",
    "        get_probe_score(word_to_probe[\"gun\"], entry, model=self_obf_model).mean().item()\n",
    "    )\n",
    "\n",
    "    results[entry_name] = {\n",
    "        \"base_model_score\": base_score,\n",
    "        \"self_obf_model_score\": obf_score,\n",
    "    }\n",
    "\n",
    "    print(f\"{entry_name}:\")\n",
    "    print(f\"  Self Obf model: {obf_score:.4f}\")\n",
    "    print(f\"  Base model: {base_score:.4f}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92369621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline comparison results\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add bars for base model\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=list(results.keys()),\n",
    "        y=[results[k][\"base_model_score\"] for k in results.keys()],\n",
    "        name=\"Base Model\",\n",
    "        marker_color=\"blue\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add bars for self-obfuscated model\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=list(results.keys()),\n",
    "        y=[results[k][\"self_obf_model_score\"] for k in results.keys()],\n",
    "        name=\"Self-Obfuscated Model\",\n",
    "        marker_color=\"red\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Gun Probe Scores: Base vs Self-Obfuscated Model\",\n",
    "    xaxis_title=\"Example Type\",\n",
    "    yaxis_title=\"Probe Score\",\n",
    "    barmode=\"group\",\n",
    "    height=500,\n",
    "    width=1000,\n",
    ")\n",
    "\n",
    "fig.write_html(f\"{RESULTS_DIR}/baseline_comparison.html\")\n",
    "fig.write_image(\n",
    "    f\"{RESULTS_DIR}/baseline_comparison.png\", width=1200, height=800, scale=2\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc0ea0",
   "metadata": {},
   "source": [
    "## 2. Layer Stitching Experiment\n",
    "\n",
    "Now let's run a layer stitching experiment to analyze where the self-obfuscation\n",
    "behavior is localized in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forward and reverse layer stitching experiments\n",
    "max_layer = LAYERS_TO_PROBE[0]\n",
    "gun_adv_gun_examples = entries[\"Gun adv gun\"]\n",
    "gun_vanilla_examples = entries[\"Gun vanilla\"]\n",
    "\n",
    "# Forward stitching (replace layers 0 to i from base model)\n",
    "res_adv_gun = []\n",
    "res_vanilla_gun = []\n",
    "for i in range(max_layer + 1):\n",
    "    stitch_model(self_obf_model, base_model, list(range(i + 1)))\n",
    "    adv_score = (\n",
    "        get_probe_score(\n",
    "            word_to_probe[\"gun\"], gun_adv_gun_examples, model=self_obf_model\n",
    "        )\n",
    "        .mean()\n",
    "        .item()\n",
    "    )\n",
    "    vanilla_score = (\n",
    "        get_probe_score(\n",
    "            word_to_probe[\"gun\"], gun_vanilla_examples, model=self_obf_model\n",
    "        )\n",
    "        .mean()\n",
    "        .item()\n",
    "    )\n",
    "    res_adv_gun.append(adv_score)\n",
    "    res_vanilla_gun.append(vanilla_score)\n",
    "    print(f\"Layer 0-{i}:\")\n",
    "    print(f\"  Adv gun probe score: {adv_score:.4f}\")\n",
    "    print(f\"  Vanilla gun probe score: {vanilla_score:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "unstitch_model(self_obf_model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa3deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse stitching (replace layers i to max_layer from base model)\n",
    "res_adv_gun_rev = []\n",
    "res_vanilla_gun_rev = []\n",
    "for i in range(max_layer, -1, -1):\n",
    "    stitch_model(self_obf_model, base_model, [i])\n",
    "    adv_score = (\n",
    "        get_probe_score(\n",
    "            word_to_probe[\"gun\"], gun_adv_gun_examples, model=self_obf_model\n",
    "        )\n",
    "        .mean()\n",
    "        .item()\n",
    "    )\n",
    "    vanilla_score = (\n",
    "        get_probe_score(\n",
    "            word_to_probe[\"gun\"], gun_vanilla_examples, model=self_obf_model\n",
    "        )\n",
    "        .mean()\n",
    "        .item()\n",
    "    )\n",
    "    res_adv_gun_rev.append(adv_score)\n",
    "    res_vanilla_gun_rev.append(vanilla_score)\n",
    "    print(f\"Layer {i}-n:\")\n",
    "    print(f\"  Adv gun probe score: {adv_score:.4f}\")\n",
    "    print(f\"  Vanilla gun probe score: {vanilla_score:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "unstitch_model(self_obf_model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b891eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special case: layers 5-6\n",
    "stitch_model(self_obf_model, base_model, [5, 6])\n",
    "layers_5_6_adv_gun = (\n",
    "    get_probe_score(word_to_probe[\"gun\"], gun_adv_gun_examples, model=self_obf_model)\n",
    "    .mean()\n",
    "    .item()\n",
    ")\n",
    "layers_5_6_vanilla_gun = (\n",
    "    get_probe_score(word_to_probe[\"gun\"], gun_vanilla_examples, model=self_obf_model)\n",
    "    .mean()\n",
    "    .item()\n",
    ")\n",
    "unstitch_model(self_obf_model, None)\n",
    "\n",
    "print(f\"Layers 5-6:\")\n",
    "print(f\"  Adv gun probe score: {layers_5_6_adv_gun:.4f}\")\n",
    "print(f\"  Vanilla gun probe score: {layers_5_6_vanilla_gun:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot layer stitching results\n",
    "fig = plot_stitching_results(\n",
    "    res_adv_gun=res_adv_gun,\n",
    "    res_vanilla_gun=res_vanilla_gun,\n",
    "    res_adv_gun_rev=res_adv_gun_rev,\n",
    "    res_vanilla_gun_rev=res_vanilla_gun_rev,\n",
    "    layers_5_6_adv_gun=layers_5_6_adv_gun,\n",
    "    layers_5_6_vanilla_gun=layers_5_6_vanilla_gun,\n",
    "    max_layer=max_layer,\n",
    "    save_dir=str(RESULTS_DIR),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab85b4",
   "metadata": {},
   "source": [
    "## 3. Layer Sweep Experiment\n",
    "\n",
    "Next, let's run a layer sweep experiment to test the impact of replacing different sets\n",
    "of consecutive layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run layer sweep experiments with different numbers of consecutive layers\n",
    "layer_sweep_results = {}\n",
    "\n",
    "for num_layers in [1, 3, 5]:\n",
    "    print(f\"Running layer sweep with {num_layers} consecutive layers...\")\n",
    "\n",
    "    # From obfuscated model to base model (adversarial examples)\n",
    "    layer_sweep_results[f\"{num_layers}_layer_from_obf_adv_gun\"] = layer_sweep(\n",
    "        base_model,\n",
    "        self_obf_model,\n",
    "        word_to_probe[\"gun\"],\n",
    "        entries[\"Gun adv gun\"],\n",
    "        num_layers,\n",
    "        max_layer,\n",
    "    )\n",
    "\n",
    "    # From base model to obfuscated model (adversarial examples)\n",
    "    layer_sweep_results[f\"{num_layers}_layer_from_base_adv_gun\"] = layer_sweep(\n",
    "        self_obf_model,\n",
    "        base_model,\n",
    "        word_to_probe[\"gun\"],\n",
    "        entries[\"Gun adv gun\"],\n",
    "        num_layers,\n",
    "        max_layer,\n",
    "    )\n",
    "\n",
    "    # From obfuscated model to base model (vanilla examples)\n",
    "    layer_sweep_results[f\"{num_layers}_layer_from_obf_vanilla_gun\"] = layer_sweep(\n",
    "        base_model,\n",
    "        self_obf_model,\n",
    "        word_to_probe[\"gun\"],\n",
    "        entries[\"Gun vanilla\"],\n",
    "        num_layers,\n",
    "        max_layer,\n",
    "    )\n",
    "\n",
    "    # From base model to obfuscated model (vanilla examples)\n",
    "    layer_sweep_results[f\"{num_layers}_layer_from_base_vanilla_gun\"] = layer_sweep(\n",
    "        self_obf_model,\n",
    "        base_model,\n",
    "        word_to_probe[\"gun\"],\n",
    "        entries[\"Gun vanilla\"],\n",
    "        num_layers,\n",
    "        max_layer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedf157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot layer sweep results\n",
    "fig = plot_layer_sweep_results(\n",
    "    layer_sweep_results=layer_sweep_results,\n",
    "    max_layer=max_layer,\n",
    "    save_dir=str(RESULTS_DIR),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3c848",
   "metadata": {},
   "source": [
    "## 4. Representation Analysis\n",
    "\n",
    "Now let's analyze the differences in representations between the base and self-obfuscated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get representation differences for adversarial gun examples\n",
    "print(\"Analyzing representations for adversarial gun examples...\")\n",
    "(\n",
    "    input_reps_norms_adv_gun,\n",
    "    target_reps_norms_adv_gun,\n",
    "    input_reps_norms_normalized_adv_gun,\n",
    "    target_reps_norms_normalized_adv_gun,\n",
    "    input_base_reps_norms_adv_gun,\n",
    "    input_obf_reps_norms_adv_gun,\n",
    "    target_base_reps_norms_adv_gun,\n",
    "    target_obf_reps_norms_adv_gun,\n",
    "    kl_div_input_adv_gun,\n",
    "    kl_div_target_adv_gun,\n",
    "    stat_dist_input_adv_gun,\n",
    "    stat_dist_target_adv_gun,\n",
    ") = get_dist_between_reps(entries[\"Gun adv gun\"], base_model, self_obf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35ee552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get representation differences for vanilla gun examples\n",
    "print(\"Analyzing representations for vanilla gun examples...\")\n",
    "(\n",
    "    input_reps_norms_vanilla_gun,\n",
    "    target_reps_norms_vanilla_gun,\n",
    "    input_reps_norms_normalized_vanilla_gun,\n",
    "    target_reps_norms_normalized_vanilla_gun,\n",
    "    input_base_reps_norms_vanilla_gun,\n",
    "    input_obf_reps_norms_vanilla_gun,\n",
    "    target_base_reps_norms_vanilla_gun,\n",
    "    target_obf_reps_norms_vanilla_gun,\n",
    "    kl_div_input_vanilla_gun,\n",
    "    kl_div_target_vanilla_gun,\n",
    "    stat_dist_input_vanilla_gun,\n",
    "    stat_dist_target_vanilla_gun,\n",
    ") = get_dist_between_reps(entries[\"Gun vanilla\"], base_model, self_obf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot representation norms\n",
    "fig_norms = plot_representation_norms(\n",
    "    input_reps_norms_adv_gun=input_reps_norms_adv_gun,\n",
    "    target_reps_norms_adv_gun=target_reps_norms_adv_gun,\n",
    "    input_reps_norms_normalized_adv_gun=input_reps_norms_normalized_adv_gun,\n",
    "    target_reps_norms_normalized_adv_gun=target_reps_norms_normalized_adv_gun,\n",
    "    input_base_reps_norms_adv_gun=input_base_reps_norms_adv_gun,\n",
    "    input_obf_reps_norms_adv_gun=input_obf_reps_norms_adv_gun,\n",
    "    input_reps_norms_vanilla_gun=input_reps_norms_vanilla_gun,\n",
    "    target_reps_norms_vanilla_gun=target_reps_norms_vanilla_gun,\n",
    "    input_reps_norms_normalized_vanilla_gun=input_reps_norms_normalized_vanilla_gun,\n",
    "    target_reps_norms_normalized_vanilla_gun=target_reps_norms_normalized_vanilla_gun,\n",
    "    input_base_reps_norms_vanilla_gun=input_base_reps_norms_vanilla_gun,\n",
    "    input_obf_reps_norms_vanilla_gun=input_obf_reps_norms_vanilla_gun,\n",
    "    save_dir=str(RESULTS_DIR),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a338da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot KL divergence and statistical distance\n",
    "fig_kl = plot_kl_divergence(\n",
    "    kl_div_input_adv_gun=kl_div_input_adv_gun,\n",
    "    kl_div_target_adv_gun=kl_div_target_adv_gun,\n",
    "    stat_dist_input_adv_gun=stat_dist_input_adv_gun,\n",
    "    stat_dist_target_adv_gun=stat_dist_target_adv_gun,\n",
    "    kl_div_input_vanilla_gun=kl_div_input_vanilla_gun,\n",
    "    kl_div_target_vanilla_gun=kl_div_target_vanilla_gun,\n",
    "    stat_dist_input_vanilla_gun=stat_dist_input_vanilla_gun,\n",
    "    stat_dist_target_vanilla_gun=stat_dist_target_vanilla_gun,\n",
    "    save_dir=str(RESULTS_DIR),\n",
    ")\n",
    "fig_kl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc943fa6",
   "metadata": {},
   "source": [
    "## 5. Token Probability Analysis\n",
    "\n",
    "Let's analyze token probabilities to understand how the models make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a66994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample entry\n",
    "sample_entry = entries[\"Gun adv gun\"].iloc[[0]]\n",
    "\n",
    "# Get model representations\n",
    "obf_reps = get_model_reps(\n",
    "    self_obf_model,\n",
    "    sample_entry,\n",
    "    layers=list(range(len(self_obf_model.model.model.layers))),\n",
    ")\n",
    "base_reps = get_model_reps(\n",
    "    base_model,\n",
    "    sample_entry,\n",
    "    layers=list(range(len(base_model.model.model.layers))),\n",
    ")\n",
    "\n",
    "# Get token probabilities\n",
    "obf_probs = unembed_probs(obf_reps.input_reps[0], self_obf_model)\n",
    "base_probs = unembed_probs(base_reps.input_reps[0], base_model)\n",
    "\n",
    "# Get tokens\n",
    "tokens = base_model.tokenizer.convert_ids_to_tokens(base_reps.input_ids[0].tolist())\n",
    "tokens = list(filter(lambda x: x != \"<|optim-location|>\", tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb047b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample prompt\n",
    "print(\"Sample prompt:\")\n",
    "print(sample_entry.iloc[0][\"prompt\"])\n",
    "print(\"\\nSample response:\")\n",
    "print(sample_entry.iloc[0][\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot token probabilities for self-obfuscated model\n",
    "fig_obf = plot_top_tokens(\n",
    "    obf_probs.cpu(),\n",
    "    tokens,\n",
    "    self_obf_model.tokenizer,\n",
    "    height=800,\n",
    "    width=1250,\n",
    "    title=\"Self-Obfuscated Model\",\n",
    "    annot_size=10,\n",
    ")\n",
    "\n",
    "# Save figures\n",
    "fig_obf.write_html(f\"{RESULTS_DIR}/top_tokens_obf.html\")\n",
    "fig_obf.write_image(\n",
    "    f\"{RESULTS_DIR}/top_tokens_obf.png\", width=1200, height=800, scale=2\n",
    ")\n",
    "fig_obf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c9c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot token probabilities for base model\n",
    "fig_base = plot_top_tokens(\n",
    "    base_probs.cpu(),\n",
    "    tokens,\n",
    "    base_model.tokenizer,\n",
    "    height=800,\n",
    "    width=1250,\n",
    "    title=\"Base Model\",\n",
    "    annot_size=10,\n",
    ")\n",
    "\n",
    "# Save figures\n",
    "fig_base.write_html(f\"{RESULTS_DIR}/top_tokens_base.html\")\n",
    "fig_base.write_image(\n",
    "    f\"{RESULTS_DIR}/top_tokens_base.png\", width=1200, height=800, scale=2\n",
    ")\n",
    "fig_base.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd1f910",
   "metadata": {},
   "source": [
    "## 6. Patching Experiment (Still work in progress, not working for now)\n",
    "\n",
    "If you have a chat template file, you can run the patching experiment to see how\n",
    "patching token activations affects probe scores.\n",
    "\n",
    "Note: This requires NNSight to be installed and a chat template file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b8802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_TEMPLATE_PATH = WORK_DIR / \"data/gemma_chat_template.jinja\"\n",
    "# Check if chat template exists\n",
    "if os.path.exists(CHAT_TEMPLATE_PATH):\n",
    "    try:\n",
    "        import nnsight\n",
    "        from nnsight import LanguageModel\n",
    "        from transformers import AutoTokenizer\n",
    "\n",
    "        print(\"Running patching experiment...\")\n",
    "\n",
    "        # Initialize nnsight models\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
    "        tokenizer.padding_side = \"right\"\n",
    "\n",
    "        nns_self_obf_model = LanguageModel(self_obf_model.model, tokenizer=tokenizer)\n",
    "        nns_base_model = LanguageModel(base_model.model, tokenizer=tokenizer)\n",
    "\n",
    "        # Run patching experiment\n",
    "        adv_samples = entries[\"Gun adv gun\"]\n",
    "        probe_score = patching_exp(\n",
    "            adv_samples,\n",
    "            word_to_probe[\"gun\"],\n",
    "            source_model=nns_base_model,\n",
    "            target_model=nns_self_obf_model,\n",
    "            tokenizer=base_model.tokenizer,\n",
    "            max_layer=LAYERS_TO_PROBE[0],\n",
    "            chat_template_path=CHAT_TEMPLATE_PATH,\n",
    "        )\n",
    "\n",
    "        print(f\"Patching experiment probe score: {probe_score}\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"NNSight not installed. Skipping patching experiment.\")\n",
    "else:\n",
    "    print(\"Chat template not found. Skipping patching experiment.\")\n",
    "    print(\n",
    "        f\"To run the patching experiment, please provide a chat template at {CHAT_TEMPLATE_PATH}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d497f02d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated the main experiments from the self-obfuscation probe testing toolkit:\n",
    "\n",
    "1. Baseline comparisons between base and self-obfuscated models\n",
    "2. Layer stitching experiments to localize self-obfuscation behavior\n",
    "3. Layer sweep experiments to analyze the impact of replacing different sets of layers\n",
    "4. Representation analysis to understand differences between models\n",
    "5. Token probability analysis to visualize how models make predictions\n",
    "6. (Optional) Patching experiment to modify specific model behaviors\n",
    "\n",
    "The results suggest that self-obfuscation is:\n",
    "\n",
    "- Localized in specific layers of the model\n",
    "- More pronounced for adversarial examples than vanilla examples\n",
    "- Characterized by significant differences in token probability distributions\n",
    "\n",
    "For more detailed analysis, check out the generated plots in the `notebook_results` directory."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
